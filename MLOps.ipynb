{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7ddfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: ['AKM1MP6P0OYPR', '0132793040', '5.0', '1365811200']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     13\u001b[0m ratings \u001b[38;5;241m=\u001b[39m ratings\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviewerID\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProductId\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m })\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Create user-item matrix\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_matrix \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProductId\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train-test split\u001b[39;00m\n\u001b[1;32m     23\u001b[0m train, test \u001b[38;5;241m=\u001b[39m train_test_split(ratings, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Load dataset\n",
    "ratings = pd.read_csv(\"/Users/mohdmaazraeen/Desktop/Sem 5/PBL/ratings_Electronics.csv\")\n",
    "print(\"Original columns:\", ratings.columns.tolist())\n",
    "\n",
    "\n",
    "# Rename columns to standard format\n",
    "ratings = ratings.rename(columns={\n",
    "    \"reviewerID\": \"userId\",\n",
    "    \"asin\": \"ProductId\",\n",
    "    \"overall\": \"rating\"\n",
    "})\n",
    "\n",
    "# Create user-item matrix\n",
    "train_matrix = train.pivot(index=\"userId\", columns=\"ProductId\", values=\"rating\").fillna(0)\n",
    "\n",
    "# Train-test split\n",
    "train, test = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "def train_recommender(n_components=20, max_iter=200):\n",
    "    # Build matrix factorization model\n",
    "    model = NMF(n_components=n_components, max_iter=max_iter, random_state=42)\n",
    "    user_factors = model.fit_transform(user_item_matrix)\n",
    "    item_factors = model.components_\n",
    "    \n",
    "    # Predict ratings for test data\n",
    "    preds, truths = [], []\n",
    "    user_map = {u: i for i, u in enumerate(user_item_matrix.index)}\n",
    "    item_map = {p: i for i, p in enumerate(user_item_matrix.columns)}\n",
    "\n",
    "    for _, row in test.iterrows():\n",
    "        u, i, r = row[\"userId\"], row[\"ProductId\"], row[\"rating\"]\n",
    "        if u in user_map and i in item_map:\n",
    "            pred = np.dot(user_factors[user_map[u], :], item_factors[:, item_map[i]])\n",
    "            preds.append(pred)\n",
    "            truths.append(r)\n",
    "\n",
    "    rmse = mean_squared_error(truths, preds, squared=False)\n",
    "    print(f\"Model trained with RMSE: {rmse:.4f}\")\n",
    "    return model\n",
    "\n",
    "trained_model = train_recommender(n_components=30, max_iter=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43d0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_recommender(train, test, n_components=20, max_iter=200):\n",
    "    # Check column names\n",
    "    print(\"Train columns:\", train.columns)\n",
    "\n",
    "    # Create user-item matrix from training data\n",
    "    train_matrix = train.pivot(index=\"userId\", columns=\"ProductId\", values=\"rating\").fillna(0)\n",
    "\n",
    "    # Build matrix factorization model\n",
    "    model = NMF(n_components=n_components, max_iter=max_iter, random_state=42)\n",
    "    user_factors = model.fit_transform(train_matrix)\n",
    "    item_factors = model.components_\n",
    "    \n",
    "    # Maps for indexing\n",
    "    user_map = {u: i for i, u in enumerate(train_matrix.index)}\n",
    "    item_map = {p: i for i, p in enumerate(train_matrix.columns)}\n",
    "\n",
    "    # Predict ratings for test data\n",
    "    preds, truths = [], []\n",
    "    for _, row in test.iterrows():\n",
    "        u, i, r = row[\"userId\"], row[\"ProductId\"], row[\"rating\"]\n",
    "        if u in user_map and i in item_map:\n",
    "            pred = np.dot(user_factors[user_map[u], :], item_factors[:, item_map[i]])\n",
    "            preds.append(pred)\n",
    "            truths.append(r)\n",
    "\n",
    "    rmse = mean_squared_error(truths, preds, squared=False)\n",
    "    print(f\"Model trained with RMSE: {rmse:.4f}\")\n",
    "    return model, user_factors, item_factors, train_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04833457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Columns after renaming: Index(['AKM1MP6P0OYPR', '0132793040', '5.0', '1365811200'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['userId', 'ProductId', 'rating'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Columns after renaming:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ratings\u001b[38;5;241m.\u001b[39mcolumns[:\u001b[38;5;241m5\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Keep only the relevant columns\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m ratings \u001b[38;5;241m=\u001b[39m ratings[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProductId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train-test split\u001b[39;00m\n\u001b[1;32m     31\u001b[0m train, test \u001b[38;5;241m=\u001b[39m train_test_split(ratings, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['userId', 'ProductId', 'rating'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Load dataset\n",
    "ratings = pd.read_csv(\"/Users/mohdmaazraeen/Desktop/Sem 5/PBL/ratings_Electronics.csv\")\n",
    "print(\"Original columns:\", ratings.columns.tolist())\n",
    "\n",
    "\n",
    "# Clean column names (strip spaces, lowercase for matching)\n",
    "ratings.columns = ratings.columns.str.strip()\n",
    "\n",
    "# Auto-map columns to standard names\n",
    "col_map = {}\n",
    "for col in ratings.columns:\n",
    "    if \"reviewer\" in col.lower():\n",
    "        col_map[col] = \"userId\"\n",
    "    elif col.lower() == \"asin\":\n",
    "        col_map[col] = \"ProductId\"\n",
    "    elif \"overall\" in col.lower():\n",
    "        col_map[col] = \"rating\"\n",
    "\n",
    "ratings = ratings.rename(columns=col_map)\n",
    "\n",
    "print(\" Columns after renaming:\", ratings.columns[:5])\n",
    "\n",
    "# Keep only the relevant columns\n",
    "ratings = ratings[[\"userId\", \"ProductId\", \"rating\"]]\n",
    "\n",
    "# Train-test split\n",
    "train, test = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "def train_recommender(train, test, n_components=20, max_iter=200):\n",
    "    # Create user-item matrix from training data\n",
    "    train_matrix = train.pivot(index=\"userId\", columns=\"ProductId\", values=\"rating\").fillna(0)\n",
    "\n",
    "    # Build matrix factorization model\n",
    "    model = NMF(n_components=n_components, max_iter=max_iter, random_state=42)\n",
    "    user_factors = model.fit_transform(train_matrix)\n",
    "    item_factors = model.components_\n",
    "    \n",
    "    # Maps for indexing\n",
    "    user_map = {u: i for i, u in enumerate(train_matrix.index)}\n",
    "    item_map = {p: i for i, p in enumerate(train_matrix.columns)}\n",
    "\n",
    "    # Predict ratings for test data\n",
    "    preds, truths = [], []\n",
    "    for _, row in test.iterrows():\n",
    "        u, i, r = row[\"userId\"], row[\"ProductId\"], row[\"rating\"]\n",
    "        if u in user_map and i in item_map:\n",
    "            pred = np.dot(user_factors[user_map[u], :], item_factors[:, item_map[i]])\n",
    "            preds.append(pred)\n",
    "            truths.append(r)\n",
    "\n",
    "    rmse = mean_squared_error(truths, preds, squared=False)\n",
    "    print(f\"ðŸ“Š Model trained with RMSE: {rmse:.4f}\")\n",
    "    return model, user_factors, item_factors, train_matrix\n",
    "\n",
    "# Train the recommender\n",
    "trained_model, user_factors, item_factors, train_matrix = train_recommender(\n",
    "    train, test, n_components=30, max_iter=300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669f5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7743af9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m user_id \u001b[38;5;241m=\u001b[39m train_matrix\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# pick a user from training data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(recommend_products(user_id, user_factors, item_factors, train_matrix, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "user_id = train_matrix.index[0]  # pick a user from training data\n",
    "print(recommend_products(user_id, user_factors, item_factors, train_matrix, top_n=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36560025",
   "metadata": {},
   "source": [
    "### Fresh Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9041c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: ['AKM1MP6P0OYPR', '0132793040', '5.0', '1365811200']\n",
      "    AKM1MP6P0OYPR  0132793040  5.0  1365811200\n",
      "0  A2CX7LUOHB2NDG  0321732944  5.0  1341100800\n",
      "1  A2NWSAGRHCP8N5  0439886341  1.0  1367193600\n",
      "2  A2WNBOD3WNDNKT  0439886341  3.0  1374451200\n",
      "3  A1GI0U4ZRJA8WN  0439886341  1.0  1334707200\n",
      "4  A1QGNMC6O1VW39  0511189877  5.0  1397433600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "ratings = pd.read_csv(\"/Users/mohdmaazraeen/Desktop/Sem 5/PBL/ratings_Electronics.csv\")\n",
    "\n",
    "# Print first few rows and column names\n",
    "print(\"Columns in dataset:\", ratings.columns.tolist())\n",
    "print(ratings.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0079c151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after renaming: ['userId', 'ProductId', 'rating']\n",
      "           userId   ProductId  rating\n",
      "0   AKM1MP6P0OYPR  0132793040     5.0\n",
      "1  A2CX7LUOHB2NDG  0321732944     5.0\n",
      "2  A2NWSAGRHCP8N5  0439886341     1.0\n",
      "3  A2WNBOD3WNDNKT  0439886341     3.0\n",
      "4  A1GI0U4ZRJA8WN  0439886341     1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset without headers\n",
    "ratings = pd.read_csv(\n",
    "    \"/Users/mohdmaazraeen/Desktop/Sem 5/PBL/ratings_Electronics.csv\",\n",
    "    header=None,\n",
    "    names=[\"userId\", \"ProductId\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "# Keep only the relevant columns\n",
    "ratings = ratings[[\"userId\", \"ProductId\", \"rating\"]]\n",
    "\n",
    "print(\"Columns after renaming:\", ratings.columns.tolist())\n",
    "print(ratings.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34dff7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39473, 24397)\n"
     ]
    }
   ],
   "source": [
    "# Take a sample to experiment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sample_ratings = ratings.sample(n=50000, random_state=42)\n",
    "\n",
    "train, test = train_test_split(sample_ratings, test_size=0.2, random_state=42)\n",
    "train_matrix = train.pivot(index=\"userId\", columns=\"ProductId\", values=\"rating\").fillna(0)\n",
    "\n",
    "print(train_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79856cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sw/mj1l3lsn4wb72kn7rbml63j00000gn/T/ipykernel_87184/761606011.py:7: PerformanceWarning: The following operation may generate 1570105544109 cells in the resulting pandas object.\n",
      "  train_matrix = train.pivot(index=\"userId\", columns=\"ProductId\", values=\"rating\").fillna(0)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset\n",
    "train, test = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create user-item matrix for training\n",
    "train_matrix = train.pivot(index=\"userId\", columns=\"ProductId\", values=\"rating\").fillna(0)\n",
    "\n",
    "print(train_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145cadc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m n_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m NMF(n_components\u001b[38;5;241m=\u001b[39mn_components, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m user_factors \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit_transform(train_matrix)\n\u001b[1;32m      9\u001b[0m item_factors \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcomponents_\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/decomposition/_nmf.py:1657\u001b[0m, in \u001b[0;36mNMF.fit_transform\u001b[0;34m(self, X, y, W, H)\u001b[0m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(assume_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1655\u001b[0m     W, H, n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, W\u001b[38;5;241m=\u001b[39mW, H\u001b[38;5;241m=\u001b[39mH)\n\u001b[0;32m-> 1657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstruction_err_ \u001b[38;5;241m=\u001b[39m _beta_divergence(\n\u001b[1;32m   1658\u001b[0m     X, W, H, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beta_loss, square_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1659\u001b[0m )\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_ \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_ \u001b[38;5;241m=\u001b[39m H\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/decomposition/_nmf.py:133\u001b[0m, in \u001b[0;36m_beta_divergence\u001b[0;34m(X, W, H, beta, square_root)\u001b[0m\n\u001b[1;32m    131\u001b[0m     res \u001b[38;5;241m=\u001b[39m (norm_X \u001b[38;5;241m+\u001b[39m norm_WH \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m cross_prod) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     res \u001b[38;5;241m=\u001b[39m squared_norm(X \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(W, H)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m square_root:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(res \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Train NMF\n",
    "n_components = 30\n",
    "model = NMF(n_components=n_components, max_iter=300, random_state=42)\n",
    "user_factors = model.fit_transform(train_matrix)\n",
    "item_factors = model.components_\n",
    "\n",
    "# Evaluate on test set\n",
    "user_map = {u: i for i, u in enumerate(train_matrix.index)}\n",
    "item_map = {p: i for i, p in enumerate(train_matrix.columns)}\n",
    "\n",
    "preds, truths = [], []\n",
    "for _, row in test.iterrows():\n",
    "    u, i, r = row[\"userId\"], row[\"ProductId\"], row[\"rating\"]\n",
    "    if u in user_map and i in item_map:\n",
    "        pred = np.dot(user_factors[user_map[u], :], item_factors[:, item_map[i]])\n",
    "        preds.append(pred)\n",
    "        truths.append(r)\n",
    "\n",
    "rmse = mean_squared_error(truths, preds, squared=False)\n",
    "print(f\"Model trained with RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_products(user_id, user_factors, item_factors, train_matrix, top_n=5):\n",
    "    if user_id not in train_matrix.index:\n",
    "        print(\"User not found in training data\")\n",
    "        return []\n",
    "\n",
    "    user_idx = train_matrix.index.get_loc(user_id)\n",
    "    scores = np.dot(user_factors[user_idx, :], item_factors)\n",
    "    scores_series = pd.Series(scores, index=train_matrix.columns)\n",
    "\n",
    "    # Remove already rated items\n",
    "    rated_items = train_matrix.loc[user_id][train_matrix.loc[user_id] > 0].index\n",
    "    scores_series = scores_series.drop(rated_items, errors=\"ignore\")\n",
    "\n",
    "    return scores_series.sort_values(ascending=False).head(top_n)\n",
    "\n",
    "# Example\n",
    "user_id = train_matrix.index[0]\n",
    "print(recommend_products(user_id, user_factors, item_factors, train_matrix, top_n=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6dc653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0b6fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: []\n",
      "Time taken for 5000x5000 matrix multiplication: 0.99 seconds\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Check if GPU is available\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs available:\", gpus)\n",
    "\n",
    "# Simple GPU computation: matrix multiplication\n",
    "size = 5000\n",
    "a = tf.random.normal((size, size))\n",
    "b = tf.random.normal((size, size))\n",
    "\n",
    "start = time.time()\n",
    "c = tf.matmul(a, b)  # This will run on GPU if available\n",
    "tf.linalg.norm(c)     # Force computation\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time taken for {size}x{size} matrix multiplication: {end - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7792f69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Speed: 225.93 Mbps\n",
      "Upload Speed: 279.84 Mbps\n",
      "Ping: 5.96 ms\n"
     ]
    }
   ],
   "source": [
    "import speedtest\n",
    "\n",
    "def check_speed():\n",
    "    st = speedtest.Speedtest()\n",
    "    st.get_best_server()  # Finds the best server based on ping\n",
    "    download_speed = st.download() / 1_000_000  # Convert from bits/s to Mbps\n",
    "    upload_speed = st.upload() / 1_000_000      # Convert from bits/s to Mbps\n",
    "    ping = st.results.ping\n",
    "\n",
    "    print(f\"Download Speed: {download_speed:.2f} Mbps\")\n",
    "    print(f\"Upload Speed: {upload_speed:.2f} Mbps\")\n",
    "    print(f\"Ping: {ping:.2f} ms\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_speed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8d552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe05eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3c65d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4038916a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --csv CSV [--model {nmf,implicit}]\n",
      "                             [--sample_size SAMPLE_SIZE]\n",
      "                             [--test_size TEST_SIZE]\n",
      "                             [--random_state RANDOM_STATE]\n",
      "                             [--min_user_ratings MIN_USER_RATINGS]\n",
      "                             [--min_item_ratings MIN_ITEM_RATINGS]\n",
      "                             [--top_users TOP_USERS] [--top_items TOP_ITEMS]\n",
      "                             [--n_components N_COMPONENTS]\n",
      "                             [--max_iter MAX_ITER] [--factors FACTORS]\n",
      "                             [--iterations ITERATIONS]\n",
      "                             [--regularization REGULARIZATION] [--alpha ALPHA]\n",
      "                             [--positive_threshold POSITIVE_THRESHOLD]\n",
      "                             [--max_eval_users MAX_EVAL_USERS] [--top_n TOP_N]\n",
      "                             [--output_dir OUTPUT_DIR] [--verbose]\n",
      "ipykernel_launcher.py: error: argument --factors: invalid int value: '/Users/mohdmaazraeen/Library/Jupyter/runtime/kernel-v3c2383fc911caf500a9241d26c5f0f8c23cc22809.json'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Recommender system training + testing pipeline with MLOps-friendly artifacts\n",
    "File: recommender_mlops_pipeline.py\n",
    "\n",
    "Features included:\n",
    "- Robust CSV loader (handles missing headers like Amazon datasets)\n",
    "- Sampling / filtering helpers to avoid huge dense pivots\n",
    "- Dense NMF pipeline (scikit-learn) for exploration / small-scale experiments\n",
    "- Sparse ALS pipeline (implicit library) for large-scale training\n",
    "- Evaluation: RMSE for explicit NMF, Precision@K / Recall@K for ranking (implicit)\n",
    "- Recommendation function for either model type\n",
    "- Save artifacts (models, mappings, metrics) to disk in `artifacts/`\n",
    "- CLI entrypoint so you can run training from terminal or CI pipeline\n",
    "\n",
    "Usage examples:\n",
    "# Small experiment with NMF (dense):\n",
    "python recommender_mlops_pipeline.py --csv /path/to/ratings_Electronics.csv --model nmf --sample_size 50000 --top_users 5000 --top_items 3000\n",
    "\n",
    "# Large run with implicit ALS (sparse):\n",
    "pip install implicit\n",
    "python recommender_mlops_pipeline.py --csv /path/to/ratings_Electronics.csv --model implicit --sample_size 200000 --factors 64 --iterations 20\n",
    "\n",
    "Requirements (suggested):\n",
    "- pandas\n",
    "- numpy\n",
    "- scipy\n",
    "- scikit-learn\n",
    "- joblib\n",
    "- implicit (optional, for large-scale ALS)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Optional: implicit (only import when needed)\n",
    "_IMPLICIT_AVAILABLE = True\n",
    "try:\n",
    "    import implicit\n",
    "except Exception:\n",
    "    _IMPLICIT_AVAILABLE = False\n",
    "\n",
    "\n",
    "# ---------------------- Utilities ----------------------\n",
    "\n",
    "def setup_logging(verbose: bool = False):\n",
    "    level = logging.DEBUG if verbose else logging.INFO\n",
    "    logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(message)s\", level=level)\n",
    "\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "# ---------------------- Data loading ----------------------\n",
    "\n",
    "def robust_load_ratings(csv_path: str, sample_size: int = None, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Robust loader for Amazon-style rating CSVs. It will try to auto-detect whether\n",
    "    the file contains a header or not and normalize column names to: userId, ProductId, rating.\n",
    "\n",
    "    Returns: pandas.DataFrame with columns ['userId','ProductId','rating']\n",
    "    \"\"\"\n",
    "    # Try reading with header inferred\n",
    "    logging.info(f\"Loading CSV (trying with header) from {csv_path}\")\n",
    "    try:\n",
    "        df_try = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"First read failed: {e}; trying with no header\")\n",
    "        df_try = pd.read_csv(csv_path, header=None)\n",
    "\n",
    "    cols_lower = [str(c).lower().strip() for c in df_try.columns]\n",
    "    # Heuristic: look for familiar column names\n",
    "    found_user = any('reviewer' in c or c == 'userid' or 'user' == c for c in cols_lower)\n",
    "    found_item = any('asin' in c or 'product' in c or 'productid' in c for c in cols_lower)\n",
    "    found_rating = any('overall' in c or 'rating' in c for c in cols_lower)\n",
    "\n",
    "    if found_user and found_item and found_rating:\n",
    "        logging.info(\"Header detected and contains expected columns. Normalizing column names.\")\n",
    "        # normalize names\n",
    "        col_map = {}\n",
    "        for orig in df_try.columns:\n",
    "            o = str(orig).lower().strip()\n",
    "            if 'reviewer' in o or 'userid' == o or (o == 'user'):\n",
    "                col_map[orig] = 'userId'\n",
    "            elif 'asin' in o or 'product' in o or 'productid' in o:\n",
    "                col_map[orig] = 'ProductId'\n",
    "            elif 'overall' in o or 'rating' in o:\n",
    "                col_map[orig] = 'rating'\n",
    "            elif 'time' in o:\n",
    "                col_map[orig] = 'timestamp'\n",
    "        df = df_try.rename(columns=col_map)\n",
    "        # Keep only relevant columns\n",
    "        keep = [c for c in ['userId', 'ProductId', 'rating'] if c in df.columns]\n",
    "        df = df[keep]\n",
    "    else:\n",
    "        # Common case for raw Amazon \"ratings\" files: no header, columns in order: userId, asin, rating, timestamp\n",
    "        logging.info(\"Header not detected or incomplete; re-reading with header=None and assigning standard names\")\n",
    "        df = pd.read_csv(csv_path, header=None, names=['userId', 'ProductId', 'rating', 'timestamp'])\n",
    "        df = df[['userId', 'ProductId', 'rating']]\n",
    "\n",
    "    # Basic cleaning\n",
    "    df = df.dropna(subset=['userId', 'ProductId', 'rating'])\n",
    "    # Ensure rating numeric\n",
    "    df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
    "    df = df.dropna(subset=['rating'])\n",
    "\n",
    "    # Optional sampling (row-wise) for quick experiments\n",
    "    if sample_size is not None and sample_size > 0 and sample_size < len(df):\n",
    "        logging.info(f\"Sampling {sample_size} rows from {len(df)}\")\n",
    "        df = df.sample(n=sample_size, random_state=random_state)\n",
    "\n",
    "    # Ensure types are strings for mapping\n",
    "    df['userId'] = df['userId'].astype(str)\n",
    "    df['ProductId'] = df['ProductId'].astype(str)\n",
    "\n",
    "    logging.info(f\"Loaded ratings shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------- Dense: prepare matrix for NMF ----------------------\n",
    "\n",
    "def prepare_dense_train_matrix(train_df: pd.DataFrame, min_user_ratings: int = 5, min_item_ratings: int = 5,\n",
    "                               top_users: int = None, top_items: int = None):\n",
    "    \"\"\"\n",
    "    Filters training dataframe to manageable user/item subsets and returns a pivoted dense matrix.\n",
    "\n",
    "    - Filters out users/items with very few interactions (min_user_ratings/min_item_ratings)\n",
    "    - Optionally keeps only top_users by activity and top_items by popularity\n",
    "\n",
    "    Returns: train_matrix (DataFrame), kept_user_ids (list), kept_item_ids (list)\n",
    "    \"\"\"\n",
    "    df = train_df.copy()\n",
    "\n",
    "    # Filter by count thresholds\n",
    "    user_counts = df['userId'].value_counts()\n",
    "    item_counts = df['ProductId'].value_counts()\n",
    "\n",
    "    users_keep = user_counts[user_counts >= min_user_ratings].index\n",
    "    items_keep = item_counts[item_counts >= min_item_ratings].index\n",
    "\n",
    "    df = df[df['userId'].isin(users_keep) & df['ProductId'].isin(items_keep)]\n",
    "\n",
    "    # Optionally keep top-K active users / items\n",
    "    if top_users is not None:\n",
    "        top_u = user_counts.loc[user_counts.index.isin(df['userId'])].nlargest(top_users).index\n",
    "        df = df[df['userId'].isin(top_u)]\n",
    "    if top_items is not None:\n",
    "        top_i = item_counts.loc[item_counts.index.isin(df['ProductId'])].nlargest(top_items).index\n",
    "        df = df[df['ProductId'].isin(top_i)]\n",
    "\n",
    "    logging.info(f\"After filtering for dense matrix: {df['userId'].nunique()} users, {df['ProductId'].nunique()} items, {len(df)} ratings\")\n",
    "\n",
    "    train_matrix = df.pivot(index='userId', columns='ProductId', values='rating').fillna(0)\n",
    "\n",
    "    logging.info(f\"Dense train_matrix shape: {train_matrix.shape}\")\n",
    "    return train_matrix\n",
    "\n",
    "\n",
    "# ---------------------- Sparse matrix builder ----------------------\n",
    "\n",
    "def build_sparse_matrix(df: pd.DataFrame):\n",
    "    \"\"\"Builds a csr_matrix (users x items) and returns maps.\n",
    "\n",
    "    Returns: train_csr, user_map, item_map, users_list, items_list\n",
    "    \"\"\"\n",
    "    users = df['userId'].unique()\n",
    "    items = df['ProductId'].unique()\n",
    "    user_map = {u: i for i, u in enumerate(users)}\n",
    "    item_map = {p: i for i, p in enumerate(items)}\n",
    "\n",
    "    # map to indices\n",
    "    user_idx = df['userId'].map(user_map).astype(np.int32)\n",
    "    item_idx = df['ProductId'].map(item_map).astype(np.int32)\n",
    "    data = df['rating'].astype(np.float32)\n",
    "\n",
    "    n_users = len(users)\n",
    "    n_items = len(items)\n",
    "\n",
    "    train_csr = csr_matrix((data, (user_idx, item_idx)), shape=(n_users, n_items))\n",
    "\n",
    "    logging.info(f\"Built sparse matrix: users={n_users}, items={n_items}, nnz={train_csr.nnz}\")\n",
    "    return train_csr, user_map, item_map, list(users), list(items)\n",
    "\n",
    "\n",
    "# ---------------------- Training: NMF (dense) ----------------------\n",
    "\n",
    "def train_nmf(train_matrix: pd.DataFrame, n_components: int = 30, max_iter: int = 300, random_state: int = 42):\n",
    "    \"\"\"Train scikit-learn NMF on dense train matrix.\n",
    "\n",
    "    Returns: model, user_factors (n_users x k), item_factors (k x n_items)\n",
    "    \"\"\"\n",
    "    model = NMF(n_components=n_components, max_iter=max_iter, random_state=random_state)\n",
    "    logging.info(\"Fitting NMF (this can take time on large matrices)...\")\n",
    "    user_factors = model.fit_transform(train_matrix.values)\n",
    "    item_factors = model.components_\n",
    "    logging.info(f\"NMF done: user_factors.shape={user_factors.shape}, item_factors.shape={item_factors.shape}\")\n",
    "    return model, user_factors, item_factors\n",
    "\n",
    "\n",
    "# ---------------------- Training: implicit ALS (sparse) ----------------------\n",
    "\n",
    "def train_implicit_als(train_csr: csr_matrix, factors: int = 50, regularization: float = 0.01, iterations: int = 15,\n",
    "                       alpha: float = 40.0):\n",
    "    \"\"\"\n",
    "    Trains implicit ALS (AlternatingLeastSquares) using the `implicit` library.\n",
    "    We convert explicit ratings into confidence values via: confidence = 1 + alpha * rating\n",
    "\n",
    "    Note: implicit expects an item-user matrix for fit.\n",
    "    \"\"\"\n",
    "    if not _IMPLICIT_AVAILABLE:\n",
    "        raise ImportError(\"The 'implicit' library is required for this function. Install with `pip install implicit`.\")\n",
    "\n",
    "    # Convert to item-user matrix and apply confidence scaling\n",
    "    logging.info(\"Preparing item-user matrix for implicit ALS (scaling by alpha)\")\n",
    "    item_user = (train_csr.T).tocsr() * alpha\n",
    "\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=factors, regularization=regularization, iterations=iterations)\n",
    "    logging.info(\"Fitting implicit ALS... this can take some time\")\n",
    "    model.fit(item_user)\n",
    "\n",
    "    logging.info(\"Implicit ALS training complete\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------- Evaluation: RMSE for explicit predictions ----------------------\n",
    "\n",
    "def evaluate_rmse(user_factors: np.ndarray, item_factors: np.ndarray, test_df: pd.DataFrame,\n",
    "                  user_map: dict, item_map: dict):\n",
    "    \"\"\"Computes RMSE only on (user,item) pairs that were seen in the training maps.\n",
    "    - user_factors shape: (n_users, k)\n",
    "    - item_factors shape: (k, n_items)\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    truths = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        u = row['userId']\n",
    "        p = row['ProductId']\n",
    "        r = row['rating']\n",
    "        if u in user_map and p in item_map:\n",
    "            ui = user_map[u]\n",
    "            pi = item_map[p]\n",
    "            pred = float(np.dot(user_factors[ui, :], item_factors[:, pi]))\n",
    "            preds.append(pred)\n",
    "            truths.append(r)\n",
    "\n",
    "    if len(truths) == 0:\n",
    "        logging.warning(\"No overlapping (user,item) pairs between train and test for RMSE evaluation.\")\n",
    "        return None\n",
    "\n",
    "    rmse = mean_squared_error(truths, preds, squared=False)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "# ---------------------- Evaluation: Ranking metrics for implicit ALS ----------------------\n",
    "\n",
    "def evaluate_precision_recall_at_k_implicit(als_model, train_csr: csr_matrix, test_df: pd.DataFrame,\n",
    "                                            user_map: dict, item_map: dict, k: int = 10, threshold: float = 4.0,\n",
    "                                            max_users: int = 1000):\n",
    "    \"\"\"\n",
    "    Compute mean Precision@K and Recall@K for implicit ALS model across users in test_df.\n",
    "    ground truth is defined as items in test_df with rating >= threshold.\n",
    "    We limit to `max_users` users to keep evaluation fast; increase as needed.\n",
    "    \"\"\"\n",
    "    # Build ground truth mapping in test for users\n",
    "    truth_by_user = defaultdict(set)\n",
    "    for _, row in test_df.iterrows():\n",
    "        if row['rating'] >= threshold:\n",
    "            if row['userId'] in user_map and row['ProductId'] in item_map:\n",
    "                ui = user_map[row['userId']]\n",
    "                pi = item_map[row['ProductId']]\n",
    "                truth_by_user[ui].add(pi)\n",
    "\n",
    "    if len(truth_by_user) == 0:\n",
    "        logging.warning(\"No ground-truth positive items found for evaluation.\")\n",
    "        return None\n",
    "\n",
    "    users_to_eval = list(truth_by_user.keys())[:max_users]\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for ui in users_to_eval:\n",
    "        # implicit's recommend returns (itemid, score) pairs\n",
    "        try:\n",
    "            recs = als_model.recommend(uid=ui, user_items=train_csr[ui], N=k, filter_already_liked_items=True)\n",
    "        except Exception:\n",
    "            # older implicit versions have different signature: model.recommend(userid, user_items, N)\n",
    "            recs = als_model.recommend(ui, train_csr[ui], N=k)\n",
    "\n",
    "        rec_items = [i for i, _ in recs]\n",
    "        gt = truth_by_user[ui]\n",
    "        if len(gt) == 0:\n",
    "            continue\n",
    "        intersect = len(set(rec_items) & gt)\n",
    "        precisions.append(intersect / float(k))\n",
    "        recalls.append(intersect / float(len(gt)))\n",
    "\n",
    "    if len(precisions) == 0:\n",
    "        return None\n",
    "\n",
    "    return float(np.mean(precisions)), float(np.mean(recalls))\n",
    "\n",
    "\n",
    "# ---------------------- Recommendation helpers ----------------------\n",
    "\n",
    "def recommend_nmf(user_id: str, user_factors: np.ndarray, item_factors: np.ndarray, train_matrix: pd.DataFrame, top_n: int = 10):\n",
    "    if user_id not in train_matrix.index:\n",
    "        raise KeyError(f\"User {user_id} not in training users\")\n",
    "    ui = train_matrix.index.get_loc(user_id)\n",
    "    scores = np.dot(user_factors[ui, :], item_factors)\n",
    "    scores_series = pd.Series(scores, index=train_matrix.columns)\n",
    "    # Drop already rated\n",
    "    rated = train_matrix.loc[user_id][train_matrix.loc[user_id] > 0].index\n",
    "    scores_series = scores_series.drop(rated, errors='ignore')\n",
    "    return scores_series.sort_values(ascending=False).head(top_n)\n",
    "\n",
    "\n",
    "def recommend_implicit(als_model, train_csr: csr_matrix, user_map: dict, items_list: list, user_id: str, top_n: int = 10):\n",
    "    if user_id not in user_map:\n",
    "        raise KeyError(f\"User {user_id} not in training users\")\n",
    "    ui = user_map[user_id]\n",
    "    try:\n",
    "        recs = als_model.recommend(ui, train_csr[ui], N=top_n, filter_already_liked_items=True)\n",
    "    except Exception:\n",
    "        recs = als_model.recommend(uid=ui, user_items=train_csr[ui], N=top_n)\n",
    "    # recs = list of (item_idx, score)\n",
    "    rec_items = [items_list[i] for i, _ in recs]\n",
    "    return rec_items\n",
    "\n",
    "\n",
    "# ---------------------- Artifact saving ----------------------\n",
    "\n",
    "def save_artifacts(output_dir: str, model_type: str, artifacts: dict):\n",
    "    \"\"\"Save model artifacts and metrics to disk.\n",
    "    artifacts is a dict that may contain: model_object, user_map, item_map, user_factors, item_factors, metrics\n",
    "    \"\"\"\n",
    "    ensure_dir(output_dir)\n",
    "\n",
    "    # save model: if joblib-friendly\n",
    "    if 'model_object' in artifacts and artifacts['model_object'] is not None:\n",
    "        try:\n",
    "            joblib.dump(artifacts['model_object'], os.path.join(output_dir, f'model_{model_type}.joblib'))\n",
    "            logging.info(\"Saved model object with joblib\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not joblib.dump model object: {e}\")\n",
    "\n",
    "    # save numpy arrays\n",
    "    if 'user_factors' in artifacts and artifacts['user_factors'] is not None:\n",
    "        np.save(os.path.join(output_dir, 'user_factors.npy'), artifacts['user_factors'])\n",
    "    if 'item_factors' in artifacts and artifacts['item_factors'] is not None:\n",
    "        np.save(os.path.join(output_dir, 'item_factors.npy'), artifacts['item_factors'])\n",
    "\n",
    "    # save mappings\n",
    "    if 'user_map' in artifacts:\n",
    "        with open(os.path.join(output_dir, 'user_map.json'), 'w') as f:\n",
    "            json.dump(artifacts['user_map'], f)\n",
    "    if 'item_map' in artifacts:\n",
    "        with open(os.path.join(output_dir, 'item_map.json'), 'w') as f:\n",
    "            json.dump(artifacts['item_map'], f)\n",
    "\n",
    "    # save metrics\n",
    "    if 'metrics' in artifacts:\n",
    "        with open(os.path.join(output_dir, 'metrics.json'), 'w') as f:\n",
    "            json.dump(artifacts['metrics'], f, indent=2)\n",
    "\n",
    "    logging.info(f\"Artifacts saved to {output_dir}\")\n",
    "\n",
    "\n",
    "# ---------------------- CLI / Orchestration ----------------------\n",
    "\n",
    "def main(args):\n",
    "    setup_logging(args.verbose)\n",
    "\n",
    "    ratings = robust_load_ratings(args.csv, sample_size=args.sample_size)\n",
    "\n",
    "    # Basic train/test split (hold-out)\n",
    "    train_df, test_df = train_test_split(ratings, test_size=args.test_size, random_state=args.random_state)\n",
    "    logging.info(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "\n",
    "    artifacts = {'model_object': None, 'user_factors': None, 'item_factors': None, 'user_map': None, 'item_map': None, 'metrics': {}}\n",
    "\n",
    "    if args.model == 'nmf':\n",
    "        # Prepare dense matrix with filtering to keep pivot size reasonable\n",
    "        train_matrix = prepare_dense_train_matrix(train_df, min_user_ratings=args.min_user_ratings,\n",
    "                                                 min_item_ratings=args.min_item_ratings,\n",
    "                                                 top_users=args.top_users, top_items=args.top_items)\n",
    "\n",
    "        model, user_factors, item_factors = train_nmf(train_matrix, n_components=args.n_components, max_iter=args.max_iter)\n",
    "\n",
    "        # Build maps from dense pivot index/columns\n",
    "        users_list = list(train_matrix.index)\n",
    "        items_list = list(train_matrix.columns)\n",
    "        user_map = {u: i for i, u in enumerate(users_list)}\n",
    "        item_map = {p: i for i, p in enumerate(items_list)}\n",
    "\n",
    "        rmse = evaluate_rmse(user_factors, item_factors, test_df, user_map, item_map)\n",
    "        logging.info(f\"NMF evaluation RMSE: {rmse}\")\n",
    "\n",
    "        artifacts.update({'model_object': model, 'user_factors': user_factors, 'item_factors': item_factors,\n",
    "                          'user_map': user_map, 'item_map': item_map, 'metrics': {'rmse': rmse}})\n",
    "\n",
    "        # example recommendation\n",
    "        example_user = users_list[0]\n",
    "        top_recs = recommend_nmf(example_user, user_factors, item_factors, train_matrix, top_n=args.top_n)\n",
    "        logging.info(f\"Top-{args.top_n} recommendations for user {example_user}:\\n{top_recs}\")\n",
    "\n",
    "    elif args.model == 'implicit':\n",
    "        if not _IMPLICIT_AVAILABLE:\n",
    "            raise ImportError(\"The 'implicit' library is not installed. Install with `pip install implicit` and retry.`\")\n",
    "\n",
    "        # For sparse, it's okay to keep many users/items but we may still subsample rows to limit RAM\n",
    "        train_csr, user_map, item_map, users_list, items_list = build_sparse_matrix(train_df)\n",
    "\n",
    "        als = train_implicit_als(train_csr, factors=args.factors, regularization=args.regularization,\n",
    "                                 iterations=args.iterations, alpha=args.alpha)\n",
    "\n",
    "        # Evaluate ranking metrics\n",
    "        precision, recall = evaluate_precision_recall_at_k_implicit(als, train_csr, test_df, user_map, item_map,\n",
    "                                                                     k=args.top_n, threshold=args.positive_threshold,\n",
    "                                                                     max_users=args.max_eval_users)\n",
    "        logging.info(f\"Implicit ALS evaluation: Precision@{args.top_n}={precision}, Recall@{args.top_n}={recall}\")\n",
    "\n",
    "        artifacts.update({'model_object': als, 'user_map': user_map, 'item_map': item_map,\n",
    "                          'metrics': {'precision': precision, 'recall': recall}})\n",
    "\n",
    "        # Example recommendation\n",
    "        example_user = users_list[0]\n",
    "        rec_items = recommend_implicit(als, train_csr, user_map, items_list, example_user, top_n=args.top_n)\n",
    "        logging.info(f\"Top-{args.top_n} implicit recommendations for user {example_user}:\\n{rec_items}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type: choose 'nmf' or 'implicit'\")\n",
    "\n",
    "    # Save artifacts\n",
    "    save_artifacts(args.output_dir, args.model, artifacts)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Recommender training pipeline (NMF & implicit ALS)')\n",
    "    parser.add_argument('--csv', type=str, required=True, help='Path to ratings CSV')\n",
    "    parser.add_argument('--model', choices=['nmf', 'implicit'], default='nmf', help='Which model to train')\n",
    "    parser.add_argument('--sample_size', type=int, default=50000, help='Row-wise sample size for fast experiments')\n",
    "    parser.add_argument('--test_size', type=float, default=0.2)\n",
    "    parser.add_argument('--random_state', type=int, default=42)\n",
    "\n",
    "    # Dense / NMF params\n",
    "    parser.add_argument('--min_user_ratings', type=int, default=5)\n",
    "    parser.add_argument('--min_item_ratings', type=int, default=5)\n",
    "    parser.add_argument('--top_users', type=int, default=5000, help='If set, keep only top-K users by activity for dense pivot')\n",
    "    parser.add_argument('--top_items', type=int, default=3000, help='If set, keep only top-K items by popularity for dense pivot')\n",
    "    parser.add_argument('--n_components', type=int, default=30)\n",
    "    parser.add_argument('--max_iter', type=int, default=300)\n",
    "\n",
    "    # Implicit params\n",
    "    parser.add_argument('--factors', type=int, default=64)\n",
    "    parser.add_argument('--iterations', type=int, default=20)\n",
    "    parser.add_argument('--regularization', type=float, default=0.01)\n",
    "    parser.add_argument('--alpha', type=float, default=40.0)\n",
    "    parser.add_argument('--positive_threshold', type=float, default=4.0, help='Rating threshold to treat as positive for ranking eval')\n",
    "    parser.add_argument('--max_eval_users', type=int, default=1000)\n",
    "\n",
    "    # Output / misc\n",
    "    parser.add_argument('--top_n', type=int, default=10)\n",
    "    parser.add_argument('--output_dir', type=str, default='artifacts')\n",
    "    parser.add_argument('--verbose', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
