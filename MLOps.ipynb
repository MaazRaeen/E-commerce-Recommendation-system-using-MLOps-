{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7ddfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: ['AKM1MP6P0OYPR', '0132793040', '5.0', '1365811200']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     13\u001b[0m ratings \u001b[38;5;241m=\u001b[39m ratings\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviewerID\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProductId\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m })\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Create user-item matrix\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_matrix \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProductId\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train-test split\u001b[39;00m\n\u001b[1;32m     23\u001b[0m train, test \u001b[38;5;241m=\u001b[39m train_test_split(ratings, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Load dataset\n",
    "ratings = pd.read_csv(\"/Users/mohdmaazraeen/Desktop/Sem 5/PBL/ratings_Electronics.csv\")\n",
    "print(\"Original columns:\", ratings.columns.tolist())\n",
    "\n",
    "\n",
    "# Rename columns to standard format\n",
    "ratings = ratings.rename(columns={\n",
    "    \"reviewerID\": \"userId\",\n",
    "    \"asin\": \"ProductId\",\n",
    "    \"overall\": \"rating\"\n",
    "})\n",
    "\n",
    "# Create user-item matrix\n",
    "train_matrix = train.pivot(index=\"userId\", columns=\"ProductId\", values=\"rating\").fillna(0)\n",
    "\n",
    "# Train-test split\n",
    "train, test = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "def train_recommender(n_components=20, max_iter=200):\n",
    "    # Build matrix factorization model\n",
    "    model = NMF(n_components=n_components, max_iter=max_iter, random_state=42)\n",
    "    user_factors = model.fit_transform(user_item_matrix)\n",
    "    item_factors = model.components_\n",
    "    \n",
    "    # Predict ratings for test data\n",
    "    preds, truths = [], []\n",
    "    user_map = {u: i for i, u in enumerate(user_item_matrix.index)}\n",
    "    item_map = {p: i for i, p in enumerate(user_item_matrix.columns)}\n",
    "\n",
    "    for _, row in test.iterrows():\n",
    "        u, i, r = row[\"userId\"], row[\"ProductId\"], row[\"rating\"]\n",
    "        if u in user_map and i in item_map:\n",
    "            pred = np.dot(user_factors[user_map[u], :], item_factors[:, item_map[i]])\n",
    "            preds.append(pred)\n",
    "            truths.append(r)\n",
    "\n",
    "    rmse = mean_squared_error(truths, preds, squared=False)\n",
    "    print(f\"Model trained with RMSE: {rmse:.4f}\")\n",
    "    return model\n",
    "\n",
    "trained_model = train_recommender(n_components=30, max_iter=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43d0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_recommender(train, test, n_components=20, max_iter=200):\n",
    "    # Check column names\n",
    "    print(\"Train columns:\", train.columns)\n",
    "\n",
    "    # Create user-item matrix from training data\n",
    "    train_matrix = train.pivot(index=\"userId\", columns=\"ProductId\", values=\"rating\").fillna(0)\n",
    "\n",
    "    # Build matrix factorization model\n",
    "    model = NMF(n_components=n_components, max_iter=max_iter, random_state=42)\n",
    "    user_factors = model.fit_transform(train_matrix)\n",
    "    item_factors = model.components_\n",
    "    \n",
    "    # Maps for indexing\n",
    "    user_map = {u: i for i, u in enumerate(train_matrix.index)}\n",
    "    item_map = {p: i for i, p in enumerate(train_matrix.columns)}\n",
    "\n",
    "    # Predict ratings for test data\n",
    "    preds, truths = [], []\n",
    "    for _, row in test.iterrows():\n",
    "        u, i, r = row[\"userId\"], row[\"ProductId\"], row[\"rating\"]\n",
    "        if u in user_map and i in item_map:\n",
    "            pred = np.dot(user_factors[user_map[u], :], item_factors[:, item_map[i]])\n",
    "            preds.append(pred)\n",
    "            truths.append(r)\n",
    "\n",
    "    rmse = mean_squared_error(truths, preds, squared=False)\n",
    "    print(f\"Model trained with RMSE: {rmse:.4f}\")\n",
    "    return model, user_factors, item_factors, train_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04833457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Columns after renaming: Index(['AKM1MP6P0OYPR', '0132793040', '5.0', '1365811200'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['userId', 'ProductId', 'rating'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Columns after renaming:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ratings\u001b[38;5;241m.\u001b[39mcolumns[:\u001b[38;5;241m5\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Keep only the relevant columns\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m ratings \u001b[38;5;241m=\u001b[39m ratings[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProductId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train-test split\u001b[39;00m\n\u001b[1;32m     31\u001b[0m train, test \u001b[38;5;241m=\u001b[39m train_test_split(ratings, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['userId', 'ProductId', 'rating'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Load dataset\n",
    "ratings = pd.read_csv(\"/Users/mohdmaazraeen/Desktop/Sem 5/PBL/ratings_Electronics.csv\")\n",
    "print(\"Original columns:\", ratings.columns.tolist())\n",
    "\n",
    "\n",
    "# Clean column names (strip spaces, lowercase for matching)\n",
    "ratings.columns = ratings.columns.str.strip()\n",
    "\n",
    "# Auto-map columns to standard names\n",
    "col_map = {}\n",
    "for col in ratings.columns:\n",
    "    if \"reviewer\" in col.lower():\n",
    "        col_map[col] = \"userId\"\n",
    "    elif col.lower() == \"asin\":\n",
    "        col_map[col] = \"ProductId\"\n",
    "    elif \"overall\" in col.lower():\n",
    "        col_map[col] = \"rating\"\n",
    "\n",
    "ratings = ratings.rename(columns=col_map)\n",
    "\n",
    "print(\" Columns after renaming:\", ratings.columns[:5])\n",
    "\n",
    "# Keep only the relevant columns\n",
    "ratings = ratings[[\"userId\", \"ProductId\", \"rating\"]]\n",
    "\n",
    "# Train-test split\n",
    "train, test = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "def train_recommender(train, test, n_components=20, max_iter=200):\n",
    "    # Create user-item matrix from training data\n",
    "    train_matrix = train.pivot(index=\"userId\", columns=\"ProductId\", values=\"rating\").fillna(0)\n",
    "\n",
    "    # Build matrix factorization model\n",
    "    model = NMF(n_components=n_components, max_iter=max_iter, random_state=42)\n",
    "    user_factors = model.fit_transform(train_matrix)\n",
    "    item_factors = model.components_\n",
    "    \n",
    "    # Maps for indexing\n",
    "    user_map = {u: i for i, u in enumerate(train_matrix.index)}\n",
    "    item_map = {p: i for i, p in enumerate(train_matrix.columns)}\n",
    "\n",
    "    # Predict ratings for test data\n",
    "    preds, truths = [], []\n",
    "    for _, row in test.iterrows():\n",
    "        u, i, r = row[\"userId\"], row[\"ProductId\"], row[\"rating\"]\n",
    "        if u in user_map and i in item_map:\n",
    "            pred = np.dot(user_factors[user_map[u], :], item_factors[:, item_map[i]])\n",
    "            preds.append(pred)\n",
    "            truths.append(r)\n",
    "\n",
    "    rmse = mean_squared_error(truths, preds, squared=False)\n",
    "    print(f\"📊 Model trained with RMSE: {rmse:.4f}\")\n",
    "    return model, user_factors, item_factors, train_matrix\n",
    "\n",
    "# Train the recommender\n",
    "trained_model, user_factors, item_factors, train_matrix = train_recommender(\n",
    "    train, test, n_components=30, max_iter=300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669f5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7743af9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m user_id \u001b[38;5;241m=\u001b[39m train_matrix\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# pick a user from training data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(recommend_products(user_id, user_factors, item_factors, train_matrix, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "user_id = train_matrix.index[0]  # pick a user from training data\n",
    "print(recommend_products(user_id, user_factors, item_factors, train_matrix, top_n=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36560025",
   "metadata": {},
   "source": [
    "### Fresh Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9041c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: ['AKM1MP6P0OYPR', '0132793040', '5.0', '1365811200']\n",
      "    AKM1MP6P0OYPR  0132793040  5.0  1365811200\n",
      "0  A2CX7LUOHB2NDG  0321732944  5.0  1341100800\n",
      "1  A2NWSAGRHCP8N5  0439886341  1.0  1367193600\n",
      "2  A2WNBOD3WNDNKT  0439886341  3.0  1374451200\n",
      "3  A1GI0U4ZRJA8WN  0439886341  1.0  1334707200\n",
      "4  A1QGNMC6O1VW39  0511189877  5.0  1397433600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "ratings = pd.read_csv(\"/Users/mohdmaazraeen/Desktop/Sem 5/PBL/ratings_Electronics.csv\")\n",
    "\n",
    "# Print first few rows and column names\n",
    "print(\"Columns in dataset:\", ratings.columns.tolist())\n",
    "print(ratings.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079c151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after renaming: ['userId', 'ProductId', 'rating']\n",
      "           userId   ProductId  rating\n",
      "0   AKM1MP6P0OYPR  0132793040     5.0\n",
      "1  A2CX7LUOHB2NDG  0321732944     5.0\n",
      "2  A2NWSAGRHCP8N5  0439886341     1.0\n",
      "3  A2WNBOD3WNDNKT  0439886341     3.0\n",
      "4  A1GI0U4ZRJA8WN  0439886341     1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset without headers\n",
    "ratings = pd.read_csv(\n",
    "    \"/Users/mohdmaazraeen/Desktop/Sem 5/PBL/ratings_Electronics.csv\",\n",
    "    header=None,\n",
    "    names=[\"userId\", \"ProductId\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "# Keep only the relevant columns\n",
    "ratings = ratings[[\"userId\", \"ProductId\", \"rating\"]]\n",
    "\n",
    "print(\"Columns after renaming:\", ratings.columns.tolist())\n",
    "print(ratings.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34dff7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39473, 24397)\n"
     ]
    }
   ],
   "source": [
    "# Take a sample to experiment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sample_ratings = ratings.sample(n=50000, random_state=42)\n",
    "\n",
    "train, test = train_test_split(sample_ratings, test_size=0.2, random_state=42)\n",
    "train_matrix = train.pivot(index=\"userId\", columns=\"ProductId\", values=\"rating\").fillna(0)\n",
    "\n",
    "print(train_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79856cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset\n",
    "train, test = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create user-item matrix for training\n",
    "train_matrix = train.pivot(index=\"userId\", columns=\"ProductId\", values=\"rating\").fillna(0)\n",
    "\n",
    "print(train_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145cadc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m n_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m NMF(n_components\u001b[38;5;241m=\u001b[39mn_components, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m user_factors \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit_transform(train_matrix)\n\u001b[1;32m      9\u001b[0m item_factors \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcomponents_\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/decomposition/_nmf.py:1657\u001b[0m, in \u001b[0;36mNMF.fit_transform\u001b[0;34m(self, X, y, W, H)\u001b[0m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(assume_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1655\u001b[0m     W, H, n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, W\u001b[38;5;241m=\u001b[39mW, H\u001b[38;5;241m=\u001b[39mH)\n\u001b[0;32m-> 1657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstruction_err_ \u001b[38;5;241m=\u001b[39m _beta_divergence(\n\u001b[1;32m   1658\u001b[0m     X, W, H, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beta_loss, square_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1659\u001b[0m )\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_ \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_ \u001b[38;5;241m=\u001b[39m H\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/decomposition/_nmf.py:133\u001b[0m, in \u001b[0;36m_beta_divergence\u001b[0;34m(X, W, H, beta, square_root)\u001b[0m\n\u001b[1;32m    131\u001b[0m     res \u001b[38;5;241m=\u001b[39m (norm_X \u001b[38;5;241m+\u001b[39m norm_WH \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m cross_prod) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     res \u001b[38;5;241m=\u001b[39m squared_norm(X \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(W, H)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m square_root:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(res \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Train NMF\n",
    "n_components = 30\n",
    "model = NMF(n_components=n_components, max_iter=300, random_state=42)\n",
    "user_factors = model.fit_transform(train_matrix)\n",
    "item_factors = model.components_\n",
    "\n",
    "# Evaluate on test set\n",
    "user_map = {u: i for i, u in enumerate(train_matrix.index)}\n",
    "item_map = {p: i for i, p in enumerate(train_matrix.columns)}\n",
    "\n",
    "preds, truths = [], []\n",
    "for _, row in test.iterrows():\n",
    "    u, i, r = row[\"userId\"], row[\"ProductId\"], row[\"rating\"]\n",
    "    if u in user_map and i in item_map:\n",
    "        pred = np.dot(user_factors[user_map[u], :], item_factors[:, item_map[i]])\n",
    "        preds.append(pred)\n",
    "        truths.append(r)\n",
    "\n",
    "rmse = mean_squared_error(truths, preds, squared=False)\n",
    "print(f\"Model trained with RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_products(user_id, user_factors, item_factors, train_matrix, top_n=5):\n",
    "    if user_id not in train_matrix.index:\n",
    "        print(\"User not found in training data\")\n",
    "        return []\n",
    "\n",
    "    user_idx = train_matrix.index.get_loc(user_id)\n",
    "    scores = np.dot(user_factors[user_idx, :], item_factors)\n",
    "    scores_series = pd.Series(scores, index=train_matrix.columns)\n",
    "\n",
    "    # Remove already rated items\n",
    "    rated_items = train_matrix.loc[user_id][train_matrix.loc[user_id] > 0].index\n",
    "    scores_series = scores_series.drop(rated_items, errors=\"ignore\")\n",
    "\n",
    "    return scores_series.sort_values(ascending=False).head(top_n)\n",
    "\n",
    "# Example\n",
    "user_id = train_matrix.index[0]\n",
    "print(recommend_products(user_id, user_factors, item_factors, train_matrix, top_n=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6dc653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0b6fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: []\n",
      "Time taken for 5000x5000 matrix multiplication: 0.99 seconds\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Check if GPU is available\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs available:\", gpus)\n",
    "\n",
    "# Simple GPU computation: matrix multiplication\n",
    "size = 5000\n",
    "a = tf.random.normal((size, size))\n",
    "b = tf.random.normal((size, size))\n",
    "\n",
    "start = time.time()\n",
    "c = tf.matmul(a, b)  # This will run on GPU if available\n",
    "tf.linalg.norm(c)     # Force computation\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time taken for {size}x{size} matrix multiplication: {end - start:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
